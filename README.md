# Proyecto ETL Usuarios Sospechosos üïµüèº‚Äç‚ôÇÔ∏è

Proyecto para automatizar el proceso ETL desde la base de datos PostgreSQL de `Hudson Hornet` hacia un Data Warehouse.

## Descripci√≥n üìù

Este proyecto utiliza Apache Airflow para ejecutar un DAG diario que extrae informaci√≥n de la base de datos PostgreSQL y la almacena en un Data Warehouse (DW). El objetivo es identificar usuarios con m√∫ltiples aplicaciones de cr√©dito y guardar estos datos en una tabla dedicada del DW.

## Estructura de los DAGs üë®üèΩ‚Äçüíª

- [`dags/dag_etl.py`](dags/dag_etl.py): Define el DAG principal `etl_suspicious_users`, que ejecuta diariamente el proceso ETL.
- [`dags/extract_load.py`](dags/extract_load.py): Contiene las funciones `extract`, `load` y `extract_and_load` para extraer datos de PostgreSQL y cargarlos en el DW.

## Clases de conexi√≥n a bases de datos üêç

- [`dags/db/connection.py`](dags/db/connection.py):
  - [`db.DatabaseConection`](dags/db/connection.py): Clase base para manejar conexiones a bases de datos.
  - [`db.PostgreConnection`](dags/db/connection.py): Extrae datos desde PostgreSQL.
  - [`db.EDWConnection`](dags/db/connection.py): Crea tablas y carga datos en el Data Warehouse.

## Configuraci√≥n de variables de entorno üõ†Ô∏è

Es necesario crear un archivo `.env` en la ra√≠z del proyecto con las variables de conexi√≥n para PostgreSQL y el Data Warehouse. Ejemplo:

```env
POSTGRE_USERNAME="usuario_postgres"
POSTGRE_PASSWORD="password_postgres"
POSTGRE_HOST="host_postgres"
POSTGRE_PORT=5432
POSTGRE_DBNAME="nombre_db_postgres"

EDW_USERNAME="usuario_dw"
EDW_PASSWORD="password_dw"
EDW_HOST="host_dw"
EDW_PORT=5432
EDW_DBNAME="nombre_db_dw"
EDW_SCHEMA="public"
EDW_TABLE_NAME="dw_suspicious_users"
```

**Recuerda agregar la informaci√≥n de tu Data Warehouse en el `.env`.**

## Recomendaci√≥n de entorno virtual üñ•Ô∏è

Se recomienda crear un entorno virtual con `venv` para instalar las dependencias necesarias:

```sh
python3 -m venv .venv
source venv/bin/activate
pip install -r requirements.txt
```

Esto asegura que todas las dependencias del proyecto se instalen correctamente y evita conflictos con otras librer√≠as del sistema.

## Ejecuci√≥n local con Docker üêã

1. Clona el repositorio y navega al directorio del proyecto.
2. Crea el archivo `.env` con las variables necesarias.
3. Ejecuta los servicios con Docker Compose:

```sh
docker-compose up
```

Esto levantar√° los servicios de Airflow, PostgreSQL y Redis. El DAG se ejecutar√° autom√°ticamente cada d√≠a y podr√°s monitorear la ejecuci√≥n desde la interfaz web de Airflow en [http://localhost:8080](http://localhost:8080).

- Usuario default: `airflow`
- Password default: `airflow`

---

Para m√°s detalles revisa los archivos:

- [dags/dag_etl.py](dags/dag_etl.py)
- [dags/extract_load.py](dags/extract_load.py)
- [dags/db/connection.py](dags/db/connection.py)
